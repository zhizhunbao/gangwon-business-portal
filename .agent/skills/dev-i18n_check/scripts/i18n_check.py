"""å›½é™…åŒ–æ£€æŸ¥å·¥å…· - æ£€æŸ¥ç¡¬ç¼–ç æ–‡æœ¬ã€ç¿»è¯‘åŒæ­¥å¹¶ç”ŸæˆæŠ¥å‘Š"""
import re
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple


# ============================================================================
# ç¡¬ç¼–ç æ–‡æœ¬æ£€æŸ¥
# ============================================================================

def is_in_translation_call(content: str, pos: int) -> bool:
    """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨ t() è°ƒç”¨çš„ç¬¬äºŒä¸ªå‚æ•°ï¼ˆfallbackï¼‰ä¸­"""
    # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„ t(
    before = content[:pos]
    t_call_start = before.rfind("t(")

    if t_call_start == -1:
        return False

    # ä» t( å¼€å§‹åˆ°å½“å‰ä½ç½®çš„å†…å®¹
    snippet = content[t_call_start:pos]

    # è®¡ç®—é€—å·æ•°é‡ï¼Œå¦‚æœæœ‰é€—å·è¯´æ˜å¯èƒ½åœ¨ç¬¬äºŒä¸ªå‚æ•°ä¸­
    comma_count = snippet.count(',')

    # å¦‚æœæœ‰è‡³å°‘ä¸€ä¸ªé€—å·ï¼Œè¯´æ˜åœ¨ç¬¬äºŒä¸ªå‚æ•°ï¼ˆfallbackï¼‰ä¸­
    return comma_count >= 1


def is_in_comment(content: str, pos: int) -> bool:
    """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨æ³¨é‡Šä¸­"""
    line_start = content.rfind('\n', 0, pos) + 1
    line = content[line_start:content.find('\n', pos)]
    
    if '//' in line and line.index('//') < (pos - line_start):
        return True
    
    block_start = content.rfind('/*', 0, pos)
    block_end = content.rfind('*/', 0, pos)
    return block_start > block_end


def find_hardcoded_text(file_path: Path) -> List[Dict]:
    """æŸ¥æ‰¾ç¡¬ç¼–ç çš„ä¸­æ–‡/éŸ©è¯­æ–‡æœ¬"""
    try:
        content = file_path.read_text(encoding='utf-8')
    except Exception:
        return []
    
    chinese_pattern = r'[\u4e00-\u9fff]+'
    korean_pattern = r'[\uac00-\ud7af\u1100-\u11ff\u3130-\u318f]+'
    
    issues = []
    
    for pattern, lang_type in [(chinese_pattern, 'chinese'), (korean_pattern, 'korean')]:
        for match in re.finditer(pattern, content):
            if is_in_translation_call(content, match.start()) or \
               is_in_comment(content, match.start()):
                continue
            
            line_num = content[:match.start()].count('\n') + 1
            line_start = content.rfind('\n', 0, match.start()) + 1
            line_end = content.find('\n', match.start())
            if line_end == -1:
                line_end = len(content)
            line_content = content[line_start:line_end].strip()
            
            issues.append({
                'type': lang_type,
                'text': match.group(),
                'line': line_num,
                'context': line_content[:200]
            })
    
    return issues


def find_chinese_fallbacks(file_path: Path) -> List[Dict]:
    """æŸ¥æ‰¾ t() è°ƒç”¨ä¸­ä½¿ç”¨ä¸­æ–‡ä½œä¸º fallback çš„æƒ…å†µ"""
    try:
        content = file_path.read_text(encoding='utf-8')
    except Exception:
        return []

    # åŒ¹é… t('key', 'ä¸­æ–‡') æˆ– t("key", "ä¸­æ–‡") æ¨¡å¼
    pattern = r"t\(['\"]([^'\"]+)['\"]\s*,\s*['\"]([^'\"]*[\u4e00-\u9fff]+[^'\"]*)['\"]"
    issues = []

    for match in re.finditer(pattern, content):
        key = match.group(1)
        fallback = match.group(2)

        line_num = content[:match.start()].count('\n') + 1
        line_start = content.rfind('\n', 0, match.start()) + 1
        line_end = content.find('\n', match.start())
        if line_end == -1:
            line_end = len(content)
        line_content = content[line_start:line_end].strip()

        issues.append({
            'key': key,
            'fallback': fallback,
            'line': line_num,
            'context': line_content[:200]
        })

    return issues


def scan_hardcoded(directory: Path) -> Dict[str, List[Dict]]:
    """æ‰«æç›®å½•ä¸­çš„ç¡¬ç¼–ç æ–‡æœ¬ï¼ˆä»… JSX/TSX ç»„ä»¶æ–‡ä»¶ï¼‰"""
    all_issues = {}
    extensions = ['.jsx', '.tsx']
    skip_patterns = ['node_modules', 'locales', '.venv', '_deprecated', 'dist', 'build']

    for ext in extensions:
        for file_path in directory.rglob(f'*{ext}'):
            if any(skip in str(file_path) for skip in skip_patterns):
                continue

            issues = find_hardcoded_text(file_path)
            if issues:
                rel_path = file_path.relative_to(directory.parent.parent)
                all_issues[str(rel_path)] = issues

    return all_issues


def scan_chinese_fallbacks(directory: Path) -> Dict[str, List[Dict]]:
    """æ‰«æç›®å½•ä¸­ä½¿ç”¨ä¸­æ–‡ fallback çš„ t() è°ƒç”¨ï¼ˆä»… JSX/TSX ç»„ä»¶æ–‡ä»¶ï¼‰"""
    all_issues = {}
    extensions = ['.jsx', '.tsx']
    skip_patterns = ['node_modules', 'locales', '.venv', '_deprecated', 'dist', 'build']

    for ext in extensions:
        for file_path in directory.rglob(f'*{ext}'):
            if any(skip in str(file_path) for skip in skip_patterns):
                continue

            issues = find_chinese_fallbacks(file_path)
            if issues:
                rel_path = file_path.relative_to(directory.parent.parent)
                all_issues[str(rel_path)] = issues

    return all_issues


# ============================================================================
# ç¿»è¯‘åŒæ­¥æ£€æŸ¥
# ============================================================================

def get_all_keys(data: dict, prefix: str = '') -> Set[str]:
    """é€’å½’è·å–æ‰€æœ‰ç¿»è¯‘é”®"""
    keys = set()
    for key, value in data.items():
        full_key = f"{prefix}.{key}" if prefix else key
        if isinstance(value, dict):
            keys.update(get_all_keys(value, full_key))
        else:
            keys.add(full_key)
    return keys


def check_translation_sync(ko_file: Path, zh_file: Path) -> Dict:
    """æ£€æŸ¥éŸ©è¯­å’Œä¸­æ–‡ç¿»è¯‘æ–‡ä»¶çš„åŒæ­¥æ€§"""
    try:
        ko_data = json.loads(ko_file.read_text(encoding='utf-8'))
        zh_data = json.loads(zh_file.read_text(encoding='utf-8'))
    except Exception:
        return None
    
    ko_keys = get_all_keys(ko_data)
    zh_keys = get_all_keys(zh_data)
    
    return {
        'missing_in_zh': sorted(list(ko_keys - zh_keys)),
        'missing_in_ko': sorted(list(zh_keys - ko_keys)),
        'total_ko': len(ko_keys),
        'total_zh': len(zh_keys)
    }


def scan_translations(base_dir: Path) -> Dict[str, Dict]:
    """æ‰«ææ‰€æœ‰ locales ç›®å½•"""
    results = {}
    skip_patterns = ['node_modules', '.venv', '_deprecated']
    
    for locales_dir in base_dir.rglob('locales'):
        if any(skip in str(locales_dir) for skip in skip_patterns):
            continue
        
        ko_file = locales_dir / 'ko.json'
        zh_file = locales_dir / 'zh.json'
        
        if ko_file.exists() and zh_file.exists():
            result = check_translation_sync(ko_file, zh_file)
            if result:
                results[str(locales_dir)] = result
    
    return results


# ============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# ============================================================================

def categorize_issues(all_issues: Dict[str, List[Dict]]) -> Dict[str, List[Tuple]]:
    """å°†é—®é¢˜åˆ†ç±»"""
    categories = {
        'enums': [],
        'constants': [],
        'helpers': [],
        'other': []
    }
    
    for file_path, issues in all_issues.items():
        file_str = str(file_path).lower()
        
        if 'enum' in file_str:
            categories['enums'].append((file_path, issues))
        elif 'constant' in file_str:
            categories['constants'].append((file_path, issues))
        elif 'helper' in file_str:
            categories['helpers'].append((file_path, issues))
        else:
            categories['other'].append((file_path, issues))
    
    return categories


def generate_report(hardcoded_issues: Dict[str, List[Dict]],
                   fallback_issues: Dict[str, List[Dict]],
                   output_file: Path) -> Tuple[int, int, int]:
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    total_hardcoded_files = len(hardcoded_issues)
    total_hardcoded_count = sum(len(issues) for issues in hardcoded_issues.values())
    total_fallback_files = len(fallback_issues)
    total_fallback_count = sum(len(issues) for issues in fallback_issues.values())

    report = []
    report.append("# å›½é™…åŒ–é—®é¢˜åˆ†ææŠ¥å‘Š\n")
    report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.append(f"\n## æ¦‚è§ˆ\n")
    report.append(f"- ç¡¬ç¼–ç æ–‡æœ¬: **{total_hardcoded_files}** ä¸ªæ–‡ä»¶ï¼Œ**{total_hardcoded_count}** å¤„é—®é¢˜\n")
    report.append(f"- ä¸­æ–‡ Fallback: **{total_fallback_files}** ä¸ªæ–‡ä»¶ï¼Œ**{total_fallback_count}** å¤„é—®é¢˜\n")
    report.append(f"\n---\n")

    # æŠ¥å‘Šä¸­æ–‡ fallback é—®é¢˜ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
    if fallback_issues:
        report.append(f"\n## âŒ ä¸­æ–‡ Fallback é—®é¢˜ (P1 - é«˜)\n")
        report.append(f"\n**è¯´æ˜**: é¡¹ç›®ä¸»è¦è¯­è¨€æ˜¯éŸ©è¯­ï¼Œfallback åº”ä½¿ç”¨éŸ©è¯­è€Œéä¸­æ–‡ã€‚\n")
        report.append(f"\nå‘ç° {total_fallback_files} ä¸ªæ–‡ä»¶å­˜åœ¨é—®é¢˜ï¼Œå…± {total_fallback_count} å¤„\n")

        for file_path, issues in sorted(fallback_issues.items()):
            report.append(f"\n### ğŸ“„ `{file_path}`\n")
            report.append(f"\nå…± {len(issues)} å¤„ä¸­æ–‡ fallback\n")

            for issue in issues[:20]:
                report.append(f"\n**è¡Œ {issue['line']}**\n")
                report.append(f"- é”®: `{issue['key']}`\n")
                report.append(f"- ä¸­æ–‡ fallback: `{issue['fallback']}`\n")
                report.append(f"- ä¸Šä¸‹æ–‡: `{issue['context']}`\n")

            if len(issues) > 20:
                report.append(f"\n... è¿˜æœ‰ {len(issues) - 20} å¤„é—®é¢˜\n")

    # æŠ¥å‘Šç¡¬ç¼–ç é—®é¢˜
    if hardcoded_issues:
        categories = categorize_issues(hardcoded_issues)

        priority_order = [
            ('other', 'ç»„ä»¶ç¡¬ç¼–ç ', 'P1 - é«˜'),
            ('enums', 'æšä¸¾å€¼ç¡¬ç¼–ç ', 'P2 - ä¸­'),
            ('constants', 'å¸¸é‡ç¡¬ç¼–ç ', 'P2 - ä¸­'),
            ('helpers', 'å·¥å…·å‡½æ•°ç¡¬ç¼–ç ', 'P3 - ä½')
        ]

        for cat_key, cat_name, priority in priority_order:
            items = categories[cat_key]
            if not items:
                continue

            report.append(f"\n## âŒ {cat_name} ({priority})\n")
            report.append(f"\nå‘ç° {len(items)} ä¸ªæ–‡ä»¶å­˜åœ¨é—®é¢˜\n")

            for file_path, issues in items:
                report.append(f"\n### ğŸ“„ `{file_path}`\n")
                report.append(f"\nå…± {len(issues)} å¤„ç¡¬ç¼–ç \n")

                for issue in issues[:10]:
                    report.append(f"\n**è¡Œ {issue['line']}** - {issue['type'].upper()}\n")
                    report.append(f"- æ–‡æœ¬: `{issue['text']}`\n")
                    report.append(f"- ä¸Šä¸‹æ–‡: `{issue['context']}`\n")

                if len(issues) > 10:
                    report.append(f"\n... è¿˜æœ‰ {len(issues) - 10} å¤„é—®é¢˜\n")

    report.append(f"\n---\n")
    report.append(f"\n## ä¿®å¤å»ºè®®\n")
    report.append(f"\n### ä¼˜å…ˆçº§è¯´æ˜\n")
    report.append(f"\n- **P1 - é«˜**: ä¸­æ–‡ fallback å’Œç»„ä»¶ç¡¬ç¼–ç ï¼Œå½±å“ç”¨æˆ·ä½“éªŒï¼Œåº”å°½å¿«ä¿®å¤\n")
    report.append(f"- **P2 - ä¸­**: æšä¸¾å’Œå¸¸é‡ç¡¬ç¼–ç ï¼Œå¯ä»¥é€æ­¥ä¼˜åŒ–\n")
    report.append(f"- **P3 - ä½**: å·¥å…·å‡½æ•°ä¸­çš„ç¡¬ç¼–ç ï¼Œå¯ä»¥ä¿æŒç°çŠ¶æˆ–åç»­ä¼˜åŒ–\n")

    report.append(f"\n### ä¿®å¤æ­¥éª¤\n")

    if fallback_issues:
        report.append(f"\n#### 1. ä¿®å¤ä¸­æ–‡ Fallbackï¼ˆè‡ªåŠ¨åŒ–ï¼‰\n")
        report.append(f"\n```bash\n")
        report.append(f"# é¢„è§ˆè¦ä¿®å¤çš„å†…å®¹\n")
        report.append(f"uv run python .agent/skills/dev-i18n_check/scripts/i18n_fix.py frontend/src frontend/src/shared/i18n/locales/ko.json\n")
        report.append(f"\n# åº”ç”¨ä¿®å¤\n")
        report.append(f"uv run python .agent/skills/dev-i18n_check/scripts/i18n_fix.py frontend/src frontend/src/shared/i18n/locales/ko.json --apply\n")
        report.append(f"```\n")

    if hardcoded_issues:
        report.append(f"\n#### 2. ä¿®å¤ç¡¬ç¼–ç æ–‡æœ¬ï¼ˆæ‰‹åŠ¨ï¼‰\n")
        report.append(f"\n1. åœ¨ `ko.json` å’Œ `zh.json` ä¸­æ·»åŠ å¯¹åº”çš„ç¿»è¯‘é”®\n")
        report.append(f"2. ä½¿ç”¨ `t('key', 'í•œêµ­ì–´ fallback')` æ›¿æ¢ç¡¬ç¼–ç æ–‡æœ¬\n")
        report.append(f"3. æµ‹è¯•éŸ©è¯­å’Œä¸­æ–‡ä¸¤ç§è¯­è¨€çš„æ˜¾ç¤º\n")
        report.append(f"4. è¿è¡Œæ£€æŸ¥å·¥å…·éªŒè¯ä¿®å¤ç»“æœ\n")

    report.append(f"\n### é‡æ–°æ£€æŸ¥\n")
    report.append(f"\n```bash\n")
    report.append(f"# ä¿®å¤åé‡æ–°æ£€æŸ¥\n")
    report.append(f"uv run python .agent/skills/dev-i18n_check/scripts/i18n_check.py frontend/src\n")
    report.append(f"```\n")

    output_file.write_text(''.join(report), encoding='utf-8')
    return total_hardcoded_count, total_fallback_count, total_hardcoded_files + total_fallback_files


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python i18n_check.py <å‰ç«¯ç›®å½•>")
        print("ç¤ºä¾‹: python i18n_check.py frontend/src")
        sys.exit(1)

    directory = Path(sys.argv[1])
    output_file = Path(__file__).parent.parent / 'I18N_ISSUES.md'

    print(f"æ‰«æç›®å½•: {directory}")
    print("=" * 80)

    # æ£€æŸ¥ä¸­æ–‡ fallback
    print("\nğŸ” æ£€æŸ¥ä¸­æ–‡ fallback...")
    fallback_issues = scan_chinese_fallbacks(directory)

    if fallback_issues:
        total_count = sum(len(v) for v in fallback_issues.values())
        print(f"âŒ å‘ç° {len(fallback_issues)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_count} å¤„ä¸­æ–‡ fallback")
    else:
        print("âœ… æœªå‘ç°ä¸­æ–‡ fallback")

    # æ£€æŸ¥ç¡¬ç¼–ç æ–‡æœ¬
    print("\nğŸ” æ£€æŸ¥ç¡¬ç¼–ç æ–‡æœ¬...")
    hardcoded_issues = scan_hardcoded(directory)

    if hardcoded_issues:
        total_issues = sum(len(v) for v in hardcoded_issues.values())
        print(f"âŒ å‘ç° {len(hardcoded_issues)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_issues} å¤„ç¡¬ç¼–ç ")
    else:
        print("âœ… æœªå‘ç°ç¡¬ç¼–ç æ–‡æœ¬")

    # æ£€æŸ¥ç¿»è¯‘åŒæ­¥
    print("\nğŸ” æ£€æŸ¥ç¿»è¯‘åŒæ­¥...")
    base_dir = directory.parent
    translation_results = scan_translations(base_dir)

    if translation_results:
        has_sync_issues = False
        for locales_dir, result in translation_results.items():
            if result['missing_in_zh'] or result['missing_in_ko']:
                has_sync_issues = True
                print(f"\nğŸ“ {locales_dir}")
                if result['missing_in_zh']:
                    print(f"  âŒ ä¸­æ–‡ç¼ºå¤± {len(result['missing_in_zh'])} ä¸ªé”®")
                if result['missing_in_ko']:
                    print(f"  âŒ éŸ©è¯­ç¼ºå¤± {len(result['missing_in_ko'])} ä¸ªé”®")

        if not has_sync_issues:
            print("âœ… æ‰€æœ‰ç¿»è¯‘æ–‡ä»¶å·²åŒæ­¥")
    else:
        print("âŒ æœªæ‰¾åˆ°ç¿»è¯‘æ–‡ä»¶")

    # ç”ŸæˆæŠ¥å‘Š
    if hardcoded_issues or fallback_issues:
        print(f"\nğŸ“ ç”ŸæˆæŠ¥å‘Š...")
        hardcoded_count, fallback_count, total_files = generate_report(
            hardcoded_issues, fallback_issues, output_file
        )
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  - é—®é¢˜æ–‡ä»¶æ•°: {total_files}")
        print(f"  - ç¡¬ç¼–ç æ€»æ•°: {hardcoded_count}")
        print(f"  - ä¸­æ–‡ fallback æ€»æ•°: {fallback_count}")
    else:
        print(f"\nâœ… æœªå‘ç°ä»»ä½•é—®é¢˜")

    print("\n" + "=" * 80)


if __name__ == '__main__':
    main()
